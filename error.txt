  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "C:\Users\divya\OneDrive\Desktop\MSAI\DL\homework3\homework\train_detection.py", line 133, in <module>
    train(**vars(parser.parse_args()))
  File "C:\Users\divya\OneDrive\Desktop\MSAI\DL\homework3\homework\train_detection.py", line 61, in train
    for img, label in train_data:
        ^^^^^^^^^^
ValueError: too many values to unpack (expected 2)



Fix:

# Training loop
for epoch in range(num_epoch):
    # Clear metrics at beginning of epoch
    for key in metrics:
        metrics[key].clear()

    model.train()

    for batch in train_data:
        img = batch['image'].to(device)
        depth = batch['depth'].to(device)
        track = batch['track'].to(device)

        # Forward pass
        logits, pred_depth = model(img)

        # Calculate losses
        seg_loss = nn.CrossEntropyLoss()(logits, track)
        depth_loss = nn.MSELoss()(pred_depth, depth)

        # Combine losses
        loss = seg_loss + depth_loss

        # Backward pass and optimization
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

        # Calculate training accuracy for segmentation
        _, preds = torch.max(logits, 1)
        accuracy = (preds == track).float().mean().item()
        metrics["train_acc"].append(accuracy)

        global_step += 1

    # Disable gradient computation and switch to evaluation mode
    with torch.inference_mode():
        model.eval()

        for batch in val_data:
            img = batch['image'].to(device)
            depth = batch['depth'].to(device)
            track = batch['track'].to(device)

            # Forward pass
            logits, pred_depth = model(img)

            # Calculate validation accuracy for segmentation
            _, preds = torch.max(logits, 1)
            accuracy = (preds == track).float().mean().item()
            metrics["val_acc"].append(accuracy)

    # Log average train and val accuracy to tensorboard
    epoch_train_acc = torch.as_tensor(metrics["train_acc"]).mean()
    epoch_val_acc = torch.as_tensor(metrics["val_acc"]).mean()

    logger.add_scalar('train_accuracy', epoch_train_acc, epoch)
    logger.add_scalar('val_accuracy', epoch_val_acc, epoch)

    # Print on first, last, every 10th epoch
    if epoch == 0 or epoch == num_epoch - 1 or (epoch + 1) % 10 == 0:
        print(
            f"Epoch {epoch + 1:2d} / {num_epoch:2d}: "
            f"train_acc={epoch_train_acc:.4f} "
            f"val_acc={epoch_val_acc:.4f}"
        )



===============================================

=============================================


Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "C:\Users\divya\OneDrive\Desktop\MSAI\DL\homework3\homework\train_detection.py", line 272, in <module>
    train(**vars(parser.parse_args()))
  File "C:\Users\divya\OneDrive\Desktop\MSAI\DL\homework3\homework\train_detection.py", line 229, in train
    logits, pred_depth = model(img)
                         ^^^^^^^^^^
  File "C:\Users\divya\AppData\Local\Programs\Python\Python312\Lib\site-packages\torch\nn\modules\module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\divya\AppData\Local\Programs\Python\Python312\Lib\site-packages\torch\nn\modules\module.py", line 1562, in _call_impl      
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\divya\OneDrive\Desktop\MSAI\DL\homework3\homework\models.py", line 289, in forward
    u1 = self.up1(b) + d2  # Skip connection
         ~~~~~~~~~~~~^~~~
RuntimeError: The size of tensor a (64) must match the size of tensor b (32) at non-singleton dimension 3



==============================================


===============================================


Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "C:\Users\divya\OneDrive\Desktop\MSAI\DL\homework3\homework\train_detection.py", line 272, in <module>
    train(**vars(parser.parse_args()))
  File "C:\Users\divya\OneDrive\Desktop\MSAI\DL\homework3\homework\train_detection.py", line 231, in train
    seg_loss = nn.CrossEntropyLoss()(logits, track)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\divya\AppData\Local\Programs\Python\Python312\Lib\site-packages\torch\nn\modules\module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\divya\AppData\Local\Programs\Python\Python312\Lib\site-packages\torch\nn\modules\module.py", line 1562, in _call_impl      
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\divya\AppData\Local\Programs\Python\Python312\Lib\site-packages\torch\nn\modules\loss.py", line 1188, in forward
    return F.cross_entropy(input, target, weight=self.weight,
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\divya\AppData\Local\Programs\Python\Python312\Lib\site-packages\torch\nn\functional.py", line 3104, in cross_entropy       
    return torch._C._nn.cross_entropy_loss(input, target, weight, _Reduction.get_enum(reduction), ignore_index, label_smoothing)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
RuntimeError: input and target batch or spatial sizes don't match: target [16, 96, 128], input [16, 3, 48, 64]
